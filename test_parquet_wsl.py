from src.config.spark_config import create_local_spark_session, stop_spark_session

print("ðŸ§ª Testando Leitura e Escrita de Arquivos no WSL...\n")

# Criar sessÃ£o
spark = create_local_spark_session("TestParquetWSL")

# Criar dados de teste
data = [
    (1, "JoÃ£o", 25, "SP", 5000.00),
    (2, "Maria", 30, "RJ", 6500.00),
    (3, "Pedro", 28, "MG", 5800.00),
    (4, "Ana", 35, "SP", 7200.00),
    (5, "Carlos", 42, "RS", 8100.00)
]

df = spark.createDataFrame(data, ["id", "nome", "idade", "estado", "salario"])

print("ðŸ“Š Dados originais:")
df.show()

# SALVAR PARQUET (isso nÃ£o funcionava no Windows!)
print("\nðŸ’¾ Salvando arquivo Parquet...")
df.write.mode("overwrite").parquet("data/bronze/funcionarios")
print("âœ… Arquivo Parquet salvo com sucesso!")

# LER PARQUET
print("\nðŸ“– Lendo arquivo Parquet do disco...")
df_lido = spark.read.parquet("data/bronze/funcionarios")
print("âœ… Arquivo lido com sucesso!")
df_lido.show()

# TRANSFORMAÃ‡Ã•ES
print("\nðŸ”„ Aplicando transformaÃ§Ãµes...")
df_transformado = df_lido.filter("idade > 28").select("nome", "idade", "salario")
df_transformado.show()

# SALVAR RESULTADO TRANSFORMADO
print("\nðŸ’¾ Salvando dados transformados...")
df_transformado.write.mode("overwrite").parquet("data/silver/funcionarios_filtrados")
print("âœ… Dados transformados salvos!")

# LER E VERIFICAR
print("\nðŸ“– Lendo dados transformados...")
df_final = spark.read.parquet("data/silver/funcionarios_filtrados")
df_final.show()

# Encerrar
stop_spark_session(spark)
print("\nðŸŽ‰ SUCESSO TOTAL! Leitura e escrita funcionando perfeitamente no WSL!")
