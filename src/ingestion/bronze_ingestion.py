"""
BRONZE LAYER - Ingest√£o Incremental
Adiciona novos dados mantendo hist√≥rico
"""

import sys
import os
from pyspark.sql.functions import current_timestamp, lit, col
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.spark_config import create_local_spark_session, get_data_paths, stop_spark_session

class BronzeIngestion:
    """Classe para ingest√£o incremental na camada Bronze"""
    
    def __init__(self, spark, paths):
        self.spark = spark
        self.paths = paths
        self.ingestion_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        
    def ingest_json_to_bronze(self, source_file, table_name):
        """Ingere arquivo JSON para Bronze (APPEND)"""
        print(f"\n{'='*70}")
        print(f"üîÑ Processando: {source_file}")
        print(f"{'='*70}")
        
        source_path = os.path.join(self.paths['raw'], source_file)
        bronze_path = os.path.join(self.paths['bronze'], table_name)
        
        df_raw = self.spark.read.option("inferSchema", "true").json(source_path)
        
        print(f"   üìä Registros lidos: {df_raw.count():,}")
        
        df_bronze = df_raw \
            .withColumn("ingestion_timestamp", current_timestamp()) \
            .withColumn("ingestion_id", lit(self.ingestion_id)) \
            .withColumn("source_file", lit(source_file)) \
            .withColumn("ingestion_date", current_timestamp().cast("date"))
        
        # APPEND - mant√©m hist√≥rico
        df_bronze.write \
            .mode("append") \
            .partitionBy("ingestion_date") \
            .parquet(bronze_path)
        
        print(f"   ‚úÖ {df_bronze.count():,} registros ADICIONADOS")
        print(f"   üíæ Modo: APPEND (mant√©m hist√≥rico)")
        print(f"   üÜî Ingestion ID: {self.ingestion_id}")
        
        return df_bronze
    
    def ingest_csv_to_bronze(self, source_file, table_name):
        """Ingere arquivo CSV para Bronze (APPEND)"""
        print(f"\n{'='*70}")
        print(f"üîÑ Processando: {source_file}")
        print(f"{'='*70}")
        
        source_path = os.path.join(self.paths['raw'], source_file)
        bronze_path = os.path.join(self.paths['bronze'], table_name)
        
        df_raw = self.spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(source_path)
        
        print(f"   üìä Registros lidos: {df_raw.count():,}")
        
        df_bronze = df_raw \
            .withColumn("ingestion_timestamp", current_timestamp()) \
            .withColumn("ingestion_id", lit(self.ingestion_id)) \
            .withColumn("source_file", lit(source_file)) \
            .withColumn("ingestion_date", current_timestamp().cast("date"))
        
        df_bronze.write \
            .mode("append") \
            .partitionBy("ingestion_date") \
            .parquet(bronze_path)
        
        print(f"   ‚úÖ {df_bronze.count():,} registros ADICIONADOS")
        print(f"   üíæ Modo: APPEND (mant√©m hist√≥rico)")
        print(f"   üÜî Ingestion ID: {self.ingestion_id}")
        
        return df_bronze
    
    def ingest_parquet_to_bronze(self, source_file, table_name):
        """Ingere arquivo Parquet para Bronze (APPEND)"""
        print(f"\n{'='*70}")
        print(f"üîÑ Processando: {source_file}")
        print(f"{'='*70}")
        
        source_path = os.path.join(self.paths['raw'], source_file)
        bronze_path = os.path.join(self.paths['bronze'], table_name)
        
        df_raw = self.spark.read.parquet(source_path)
        
        print(f"   üìä Registros lidos: {df_raw.count():,}")
        
        df_bronze = df_raw \
            .withColumn("ingestion_timestamp", current_timestamp()) \
            .withColumn("ingestion_id", lit(self.ingestion_id)) \
            .withColumn("source_file", lit(source_file)) \
            .withColumn("ingestion_date", current_timestamp().cast("date"))
        
        df_bronze.write \
            .mode("append") \
            .partitionBy("ingestion_date") \
            .parquet(bronze_path)
        
        print(f"   ‚úÖ {df_bronze.count():,} registros ADICIONADOS")
        print(f"   üíæ Modo: APPEND (mant√©m hist√≥rico)")
        print(f"   üÜî Ingestion ID: {self.ingestion_id}")
        
        return df_bronze
    
    def run_full_ingestion(self):
        """Executa ingest√£o incremental completa"""
        print("\n" + "="*70)
        print("üöÄ INICIANDO INGEST√ÉO BRONZE INCREMENTAL")
        print("="*70)
        print(f"üÜî Ingestion ID: {self.ingestion_id}")
        print("üíæ Modo: APPEND (mant√©m hist√≥rico de todas as execu√ß√µes)")
        print("="*70)
        
        start_time = datetime.now()
        
        self.ingest_json_to_bronze("customers.json", "bronze_customers")
        self.ingest_csv_to_bronze("products.csv", "bronze_products")
        self.ingest_json_to_bronze("sales.json", "bronze_sales")
        self.ingest_parquet_to_bronze("payments.parquet", "bronze_payments")
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print("\n" + "="*70)
        print("‚úÖ INGEST√ÉO BRONZE INCREMENTAL CONCLU√çDA!")
        print("="*70)
        print(f"   ‚è±Ô∏è  Tempo: {duration:.2f} segundos")
        print(f"   üÜî Ingestion ID: {self.ingestion_id}")
        print("="*70 + "\n")

if __name__ == "__main__":
    spark = create_local_spark_session("BronzeIngestion")
    paths = get_data_paths()
    
    ingestion = BronzeIngestion(spark, paths)
    ingestion.run_full_ingestion()
    
    print("\nüìä TOTAL DE REGISTROS (TODAS AS INGEST√ïES):")
    print("="*70)
    for table in ["bronze_customers", "bronze_products", "bronze_sales", "bronze_payments"]:
        table_path = os.path.join(paths['bronze'], table)
        if os.path.exists(table_path):
            df = spark.read.parquet(table_path)
            total = df.count()
            unique_ingestions = df.select("ingestion_id").distinct().count()
            print(f"   üì¶ {table}:")
            print(f"      ‚Ä¢ Total: {total:,} registros")
            print(f"      ‚Ä¢ Ingest√µes: {unique_ingestions} execu√ß√µes")
    print("="*70 + "\n")
    
    stop_spark_session(spark)
