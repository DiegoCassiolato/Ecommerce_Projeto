"""
ConfiguraÃ§Ã£o do Spark para execuÃ§Ã£o local
Otimizado para desenvolvimento em mÃ¡quina local
"""
from pyspark.sql import SparkSession
import os

def configure_spark_with_delta_pip(builder: SparkSession.builder) -> SparkSession.builder:
    """Configura Spark (sem Delta Lake por enquanto)"""
    print("ğŸ”§ Configurando Spark...")
    
    # Delta Lake pode ser habilitado depois se necessÃ¡rio
    # Por enquanto, use Parquet para desenvolvimento
    
    return builder

def create_local_spark_session(app_name: str) -> SparkSession:
    """Cria uma Spark Session local com configuraÃ§Ãµes otimizadas"""
    print(f"\nğŸ”§ Criando Spark Session: {app_name}...")
    
    builder = (
        SparkSession.builder
        .appName(app_name)
        .master("local[*]")
        .config("spark.driver.memory", "2g")
        .config("spark.executor.memory", "2g")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        .config("spark.kryoserializer.buffer.max", "512m")
    )
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    return spark

def get_data_paths():
    """
    Retorna os caminhos para as camadas de dados
    
    Returns:
        dict: DicionÃ¡rio com caminhos das camadas
    """
    # Pega o diretÃ³rio raiz do projeto (2 nÃ­veis acima deste arquivo)
    base_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "data")
    
    paths = {
        "raw": os.path.join(base_path, "raw"),
        "bronze": os.path.join(base_path, "bronze"),
        "silver": os.path.join(base_path, "silver"),
        "gold": os.path.join(base_path, "gold"),
        "export": os.path.join(base_path, "export")
    }
    
    # Criar diretÃ³rios se nÃ£o existirem
    for layer, path in paths.items():
        os.makedirs(path, exist_ok=True)
        print(f"   ğŸ“ {layer.capitalize()}: {path}")
    
    return paths

def stop_spark_session(spark):
    """
    Para a sessÃ£o Spark de forma segura
    
    Args:
        spark (SparkSession): SessÃ£o Spark a ser encerrada
    """
    if spark:
        spark.stop()
        print("ğŸ›‘ Spark Session encerrada")

# Teste do mÃ³dulo
if __name__ == "__main__":
    print("ğŸ§ª Testando configuraÃ§Ã£o do Spark...\n")
    
    # Criar sessÃ£o
    spark = create_local_spark_session("TestConfig")
    
    # Obter caminhos
    print("\nğŸ“‚ Caminhos configurados:")
    paths = get_data_paths()
    
    # Teste simples
    print("\nğŸ§ª Executando teste simples...")
    df_test = spark.createDataFrame([(1, "teste"), (2, "spark")], ["id", "valor"])
    df_test.show()
    
    # Encerrar
    stop_spark_session(spark)
    print("\nâœ… Teste concluÃ­do com sucesso!")
